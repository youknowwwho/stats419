---
title: 'R Notebook: IMDB (intro to graph ranks, movie rank)'
output:
  html_document:
    df_print: paged
    toc: true
    toc_float: true
    toc_depth: 5
    fig_caption: true
    number_sections: true 
---

```{r}

library(devtools);

library(humanVerseWSU);

path.github = "https://raw.githubusercontent.com/MonteShaffer/humanVerseWSU/master/";

include.me = paste0(path.github, "misc/functions-nlp.R");
source_url( include.me );
include.me = paste0(path.github, "misc/functions-nlp-str.R");
source_url( include.me );
include.me = paste0(path.github, "misc/functions-nlp-stack.R");
source_url( include.me );
include.me = paste0(path.github, "misc/functions-nlp-pos.R");
source_url( include.me );

include.me = paste0(path.github, "humanVerseWSU/R/functions-encryption.R");
source_url( include.me );



path.to.nascent = "C:/Users/Alexander Nevsky/Dropbox/WSU-419/Fall 2020/__student_access__/unit_02_confirmatory_data_analysis/nascent/";

folder.nlp = "nlp/";
path.to.nlp = paste0(path.to.nascent, folder.nlp);


###### UPDATES TO dataframe subset function ######
# inflation adjustments for NA ... and improvements on subsetting
include.me = paste0(path.github, "humanVerseWSU/R/functions-dataframe.R");
source_url( include.me );

include.me = paste0(path.github, "humanVerseWSU/R/functions-inflation.R");
source_url( include.me );

```

# (IMDB) Social Network

## Ranking an Adjacency Matrix

Ever wonder how the Google Page Rank algorithm works, well you are about to see.  The classic paper (1999) was never published, and explained the idea as a "popular vote" and the web user as a random surfer.  As we will soon see, it is an eigenvector measure of a special type of matrix, an adjacency matrix.

```{r}
myV = c(0,0,0,0,0,0,0,0,0,0,
        0,0,0,0,0,0,0,0,0,0,
      	0,0,0,0,0,0,0,0,0,0,
      	0,1,1,0,0,0,0,0,0,0,
      	1,1,0,0,0,0,0,0,0,0,
      	0,0,0,0,0,0,0,0,0,0,
      	1,1,0,0,1,1,0,0,0,0,
      	1,0,0,1,0,1,0,0,0,0,
      	0,0,1,0,0,0,0,0,0,0,
      	0,0,0,0,0,1,0,1,0,0);

myA = matrix(myV, nrow=10, byrow=TRUE);
  rownames(myA) = colnames(myA) = paste0("P.",1:10);
myA;
```
Much of what I describe can be found in my dissertation:  <http://www.mshaffer.com/arizona/dissertation/mjsPRINT.pdf>.  I will include some screenshots of some relevant descriptions.

The Google method does not require sorting (but it does no harm).  The method I use (thanks to some brilliant Italian math colleagues) does require sorting.

### Row sort
```{r}
# row sort
rs = rowSums(myA);
rs.s = sort(rs);
rs.idx = as.numeric( gsub("P.","",names(rs.s)) );

myA.s = myA[rs.idx,];
myA.s;
```
### Then, Column sort
```{r}
# col sort
cs = colSums(myA.s);
cs.s = sort(cs);
cs.idx = as.numeric( gsub("P.","",names(cs.s)) );

myA.ss = myA.s[,cs.idx];
myA.ss;


```
### Not exactly, joint sort
The two-stage approach doesn't work exactly, it doesn't maintain the adjacency feature.  A row/col should represent the same index.  Below is a correct sorting solution.

Working on a general solution: <https://stackoverflow.com/questions/65046519/>
```{r}
order = c(1,2,3,6,4,5,8,7,9,10);
myA.f = myA[order,order];
myA.f;
```

## Relationship to a Network Graph

A network graph is a mathematics define entity relationships linked in an ecosystem.

<IMG src='graph-rank/graph-pg218.png' style="border: 2px black solid;" />
<div>**Source: Dissertation, pg. 218**</div>

In this example, we have what is called a directed graph.  Each circle represents an entity or node.  In this case, the represent patents.  The direction of the arrow represents the nature of the relationship between the two nodes.  For example $P_7$ links to ${P_6, P_2, P_5}$.  In patents, this link is a legally-required reference to "prior-art" that the new patent builds upon.  

Three patents (1, 2, 6) have three in-bound links (or votes).  Both 1 & 2 share patents (5 & 7) as two of those links.  (1) also has a link from (8) and (2) has a link from (5) which is also linked to (7).  Ergo, (2) should be the winner.  Unless that recursive vote should be discounted because (7) has no incoming votes.  So maybe Google's hackathon approach is "better".  **What do you think?**  Examining this graphic, which node is "best"?


In this time-constrained graph, we have a triangular adjacency matrix (no data in the top-right, see first matrix before sorting).  **Note:** It is common to definitionally say that a node does not link to itself; the diagonal of the matrix is all zeroes.

### Adjacency Matrix
<IMG src='graph-rank/adjacency.png' style="border: 2px black solid;" />
<div>**Source: Dissertation, pg. 218**  The rows represent the "backward links" or citations.  The columns represent the "forward links" or citations.  This square matrix is a representation of the graph with nodes and arrows.</div>

### Network Graph Techniques
Google applies this same technique to web pages linking to other web pages.  A link is like a vote in a popularity contest, but not all votes are equal.  The nature of the interactions of the entire network determines the overall value of any given node.  This is what I will demonstrate today.

There are other graph approaches.  Multigraph, Shannon's Entropy, and others.  The accomplish similar ideas.  Shannon's entropy was developed in the 1950s before we had any real computing power to ascertain the optimal setup of the old "party-line telephone system".  I have used it to simultaneously rank both the nodes and the links.  A multiclass approach is a research interest of mine:  mingle the networks of patents, inventors, firms, and so on.  In the IMDB data setup, it would be similar:  movies, actors (creatives), production companies, and so on.  A network of networks.  This solution is an iterative one and has been developed by Francesco Romani and Gianna Del Corso.  The basic solution they developed in 2008, and I contacted them to help me perform the computations.  At the time, I had limited computing power.  The network graph had over 5 million nodes.  You can't do that in R.

### Row Stochastic

For the mathematics of eigenvector centrality to work on an adjacency matrix (the rows and columns have the same descriptor), each matrix row must be "stochastic"  This can be accomplished in several ways.

<IMG src='graph-rank/google.png' style="border: 2px black solid;" />
<div>**Source: Dissertation, pg. 220**</div>


#### Google's random surfer, add a little noise
For simplicity of comparison, we will use the sorted matrix.

<https://en.wikipedia.org/wiki/PageRank>

```{r}
M = myA.f;
rs = rowSums(M);
rs.zeroes = as.numeric( which(rs == 0) );
M;
```
##### Hack 1: dangling nodes
The first Google hack is to replace rows with all zeroes with all ones.  This addresses the issue of an isolate in the network, called a dangling node.

```{r}
ones = rep(1, times=10);

M.d = M;
M.d[rs.zeroes,] = ones;
M.d;
```
##### Hack 2: row normalize
This is a common procedure, so "hack" may be a bit harsh.  We divide each row by its sum.

```{r}
M.dr = M.d / rowSums(M.d);
M.dr;
```
##### Hack 3: scaling
In the random surfer setup, there was some factor of 0.15 related to some assumed probability, so the scaling factor $\alpha = 1-0.15 = 0.85$.

```{r}
alpha = 0.85;
M.drs = M.dr * alpha;
M.drs;
```
##### Hack 4: irreducibility
The eigenvector computation must converge.  When it doesn't, it suffers from an "irreducibility" problem.  Google addresses this by adding a small element to every cell in the matrix.

Final we have a row-stochastic matrix that can be evaluated.
```{r}
i.factor = (1-alpha)/nrow(M);  
M.drsi = M.drs + matrix(i.factor, nrow=10, ncol=10);
P.g = M.drsi;
P.g;
```

#### Supernode approach

<IMG src='graph-rank/supernode.png' style="border: 2px black solid;" />
<div>**Source: Dissertation, pg. 219**</div>

An alternative to all of the above, is to simply introduce a super-node into the network.  This is a bi-directional link (arrows going both ways).  In the patent space, it could represent the U.S. Patent Office, the gatekeeper of patents.  In the web space, today it could represent Google who dominates search.  In the IMDB space, it could represent the latent concept of "Hollywood."

##### Augment Matrix
```{r}
M = myA.f;
cn = rownames(M);

M.s = matrix(0, nrow=11, ncol=11);
M.s[1,2:11] = 1;
M.s[2:11,1] = 1;
M.s[2:11,2:11] = M;

rownames(M.s) = colnames(M.s) = c("P.0", cn);
M.s;
```

##### Row normalize

```{r}
P.s = M.s / rowSums(M.s);
P.s;
```

We are good to go.

### Compute Eigenvector Network Centrality

#### Power Approach
<https://en.wikipedia.org/wiki/PageRank#Power_method>
The easiest way to compute is to multiply the matrix by itself a lot of times.  Let's give it a try:

##### Google
```{r}
matrixPower = function(M, times=1)
  {
  for(i in 1:times)
    {
    M = M %*% M;
    }
  M;
  }

P.g10 = matrixPower(P.g,10);
P.g10;
P.g.eigen = P.g10[1,]; # any row
P.g.eigen;
```

##### Supernode
```{r}
P.s10 = matrixPower(P.s,10);
P.s10;
P.s.eigen = P.s10[1,]; # any row
P.s.eigen;
```
##### Equal?
Since my dissertation, I have worked with a mathematician (Mirek) to demonstrate that these are equal (under certain conditions).  Let's just see how they correlate.  Remember, we added the supernode, so let's drop that result to compare the original 10 scores across the two methods.

```{r}
cor(P.g.eigen, P.s.eigen[-c(1)]);
```
Mathematics is a strange thing?

#### Linear Solution (Instead of Power)
In the real world, we have very large sparse matrices, lots of zeroes.  I reached out to the Italian mathematicians because I felt their solution was more robust and elegant.  They only have to solve the linear system of one sub-block of the data, use substitutions for the other sub-blocks, and we have a solution.

This process is explained on pg. 165 of the dissertation <http://www.mshaffer.com/arizona/dissertation/mjsPRINT.pdf>

```{r}
M;
P.s;
```

We have lots of zeroes.  We can classify them into blocks.  Recall that columns represent forward-citations or links.  Well, ${P_7, P_9, P_{10}$ don't have any of those.  So let's just assign them a minimal "trivial" score, we chose one (1) for these "DUDS".

We solve the matrix ${P_4, P_5, P_8}$ which is described in the dissertation:

```{r}
sn = c("P.4","P.5","P.8");
sidx = 6:8;

# pg. 166 is example of computation
# pg. 80 [EQN 3.6 ... 3.10]  explains the why
R.bar = t(P.s[sidx,sidx]); # R.bar is a block partition
LHS = diag(1,nrow=3) - R.bar;
LHS;

sn = c("P.7","P.9","P.10");
sidx = 9:11;
P.s.dud = P.s[sidx,sidx];
pi.7910 = rep(1, times=3);
pi.7910;

T.bar = t(P.s[9:11, 6:8]);  # T.bar is a block partition
one.col3 = matrix(1, ncol=1, nrow=3);

RHS = one.col3 + T.bar %*% one.col3;
RHS;

pi.458 = solve(LHS,RHS);  # or "inv" function
pi.458;

# dangling nodes
one.col4 = matrix(1, ncol=1, nrow=4);
pi.1236 = one.col4 + t(P.s[6:8, 2:5]) %*% pi.458 + t(P.s[9:11, 2:5])  %*% one.col3;

pi = c(pi.1236, pi.458, pi.7910);
pi;
# 
# cor(pi, P.g.eigen);
# cor(pi, P.s.eigen[-c(1)]);
```
#### Scaling Final Answers

```{r}
myResults = cbind(( 100 * pi / max(pi)), ( as.numeric( 100 * P.g.eigen / max(P.g.eigen))), (as.numeric( 100 * P.s.eigen[-c(1)] / max(P.s.eigen[-c(1)]))));
colnames(myResults) = c("Supernode.linear", "Google.power", "Supernode.power");
rownames(myResults) = rownames(P.g);
myResults;
cor(myResults);

#order = as.numeric(gsub("P.","",rownames(myResults)));
# c(1,2,3,6,4,5,8,7,9,10);
order = c(1,2,3,5,6,4,8,7,9,10);
myResults.o = myResults[order,];
myResults.o;
```

If you understand the vote-weighting idea, you should be able to review the graphic, and understand why the results are returning the way they are.

<IMG src='graph-rank/graph-pg218.png' style="border: 2px black solid;" />
<div>**Source: Dissertation, pg. 218**</div>

Regardless, the concept of a Supernode is more intuitive than adding a bunch of small elements to a matrix (and altering the sparse matrix into a full-sized matrix).

Albeit a bit more "algebra" at setup, this solution is more precise as it does not rely on asymptotics of a power-convergence.

#### Very Large Matrices
For larger matrices, computing the power and iterating is a complicated task, but if you understand the nature of matrix multiplication, this can be parallel-processed.  It does have some computational advantages (Google can compute a power-matrix of a 10-billion node network; it originally parallel-processed on simple desktop computers daisy-chained).

Google hacks with assumptions up front, the Super-node approach requires some linear algebra and the inverse of one much smaller matrix.  Albeit nontrivial, once you develop the algorithm and function, this approach is also efficient.  As demonstrated in several papers by my colleagues.


## GRIMM stories and words

We can take any matrix that has entities and attributes (e.g., rows = stories of GRIMM, cols = features [forest, wolves, and other words]) and identify them as either nodes in the graph or links.

```{r}

path.to.nascent = "C:/Users/Alexander Nevsky/Dropbox/WSU-419/Fall 2020/__student_access__/unit_02_confirmatory_data_analysis/nascent/";

folder.nlp = "nlp/";
path.to.nlp = paste0(path.to.nascent, folder.nlp);

########## load data ##########


gutenberg.id = 2591;

path.to.gutenberg = paste0(path.to.nlp,"_data_/gutenberg/");
  createDirRecursive(path.to.gutenberg);
path.to.grimm = paste0(path.to.gutenberg,gutenberg.id,"/");
  createDirRecursive(path.to.grimm);


local.data.path = path.to.gutenberg; # currently required by grabHTML ... TODO: fix


txt.file.remote = "https://www.gutenberg.org/files/2591/2591-0.txt";
html.file.remote = "https://www.gutenberg.org/files/2591/2591-h/2591-h.htm";

df.grimm = parseGutenberg.GRIMM(path.to.grimm,
                        file.stem = "fairytales",
                        txt.file.remote = txt.file.remote,
                        html.file.remote =html.file.remote,
                        my.local.path = path.to.gutenberg);
```
### Load up GRIMM "custom"

```{r}
my.df = summarizeCustom(which="ALL", df.grimm);

my.df;
```

### EigenVectorRank

Rather than talking about a form (Google's PageRank vs. the Supernode PatentRank), let's just call it EigenVectorRank:  network centrality of an adjacency matrix using an eigenvector calculation.

```{r}

matrixPower = function(M, times=1)
  {
  for(i in 1:times)
    {
    M = M %*% M;
    }
  M;
  }

eigenVectorRank = function(AB, method="super", compute="power", compute.iter=8)
  {
  nr = nrow(AB);  rn = rownames(AB);
  nc = ncol(AB);  cn = colnames(AB);
  result = list();
  timings = list();
  AA = BB = NULL;
  print("Preparing matrices ... ");
  timer.start = as.numeric(Sys.time());
  if(nr != nc)
    {
    # let's do both ...
    print("Analyzing both ... ");
    AA = (AB) %*% t(AB);  # rows
    BB = t(AB) %*% (AB);  # cols
    } else { AA = AB; }
  timer.end = as.numeric(Sys.time());
  elapsed = round(timer.end - timer.start,2);
  print(paste0("Time: ", elapsed, " secs"));
  timings$prep = elapsed;
  
  ## AA
  print("Computing AA ... rows");
  timer.start = as.numeric(Sys.time());
  rownames(AA) = colnames(AA) = rn;
  # append supernode to END, easier
  AA=rbind(AA,1);
  AA=cbind(AA,1);
  AA[(nr+1),(nr+1)] = 0;
  diag(AA) = 0;  # nodes don't link to themselves ...
  AAn = AA / rowSums(AA); # row normalize
  AAnp = matrixPower(AAn, compute.iter);
  
  A.Rank = AAnp[1,-c((nr+1))]; # any row, less the supernode, which will have a very large value...
  names(A.Rank) = rn;
  A.Rank = sort((100*A.Rank/max(A.Rank)),decreasing=TRUE);
  
  A.Rank.df = as.data.frame( cbind( names(A.Rank), as.numeric(A.Rank) ));
    colnames(A.Rank.df) = c("id", "eigenVectorRank");
  A.Rank.df$eigenVectorRank = round(as.numeric(A.Rank.df$eigenVectorRank),2);
  result$rows = A.Rank.df;
  timer.end = as.numeric(Sys.time());
  elapsed = round(timer.end - timer.start,2);
  print(paste0("Time: ", elapsed, " secs"));
  timings$rows = elapsed;
  
  if(!is.null(BB))
  {
  ## BB
  print("Computing BB ... columns");
  timer.start = as.numeric(Sys.time());  
  rownames(BB) = colnames(BB) = cn;
  # append supernode to END, easier
  BB=rbind(BB,1);
  BB=cbind(BB,1);
  BB[(nr+1),(nr+1)] = 0;
  diag(BB) = 0;  # nodes don't link to themselves ...
  BBn = BB / rowSums(BB); # row normalize
  BBnp = matrixPower(BBn, compute.iter);
  
  B.Rank = BBnp[1,-c((nr+1))]; # any row, less the supernode, which will have a very large value...
  names(B.Rank) = cn;
  B.Rank = sort((100*B.Rank/max(B.Rank)),decreasing=TRUE);
  
  B.Rank.df = as.data.frame( cbind( names(B.Rank), as.numeric(B.Rank) ));
    colnames(B.Rank.df) = c("id", "eigenVectorRank");
  B.Rank.df$eigenVectorRank = round(as.numeric(B.Rank.df$eigenVectorRank),2);
  result$columns = B.Rank.df;
  timer.end = as.numeric(Sys.time());
  elapsed = round(timer.end - timer.start,2);
  print(paste0("Time: ", elapsed, " secs"));
  timings$columns = elapsed;
  }
  
  result$timings = timings;
  result;
  }

```

Remember, it needs to be a matrix, so let's try this.

```{r}
grimmRanks =   eigenVectorRank( as.matrix(my.df) );

grimmRanks$rows;
grimmRanks$columns;
```
This computes fairly quickly, as the matrix is not large.

Given my perceptions of GRIMM stories (I created the custom list), "THE JUNIPER TREE" best represents an "archtype" of this collection of stories.
<https://en.wikipedia.org/wiki/Archetype>

```{r}
sum(my.df$or);
sum(my.df$and);
```

Examining the words, "or" and "and" are top rated, but "or" is higher rated in context of all other GRIMM stories (not merely reviewed in isolation by counts).  Remember, "row-normalization" levels the playing field by proportioning any row.  In this example, the "and" row is proportioned by its sum so its on a level playing field with other words, such as "or".  That is, "or" appears [proportionally] in stories with more GRIMM custom words, so gets ranked higher.  


## IMDB

```{r}
library(imdb);
packageVersion("imdb");  # ‘0.1.1’

imdb::loadDataIMDB();
names(imdb.data);
```

What are the best movies of all time?  Of the modern era?  What about best actors in the same criteria.

R starts to perform really poorly when a matrix gets over 1000 rows or columns.  I did a "C++" hack in the other notebook (not required).  I did not set a limit for "parallel" (don't know how to do that at the moment), so my computer really crawled.  It took about 3 hours to perform one of the matrix Power computations.  Using an I-9900K processor.

### Fish in a barrel

When we examine Will vs. Denzel side-by-side as trees, we may be missing their context in the forest.  Are they two fish in a barrel, or two fish in a larger pond.  YES, I am mixing metaphors which is bad linguistics, but I am a data analyst, so I believe I can get away with it.

As you prepare your final analysis, you should put these actors in context of a bigger world.  I will show two larger frameworks.  Maybe compute and rank values in these larger frameworks, or compute Z-scores within the bigger pond, and review the relative performances of Will vs. Denzel in that context.

#### Last 40 years (Modern Era)

Each year, there are top-50 most popular movies.  This determines the network.

```{r}
df = subsetDataFrame(imdb.data$movies$popular50, "year", ">=", 1980);
df = subsetDataFrame(df, "year", "<", 2020);
ttids = df$ttid;
dim(df);

df.cast = merge(df, imdb.data$movies.df$cast, by="ttid");
dim(df.cast);
length(unique(df.cast$ttid));
length(unique(df.cast$nmid));

network = df.cast;
n.ttid = length(unique(network$ttid));
n.nmid = length(unique(network$nmid));

my.ttids = sort( unique(network$ttid) );
my.nmids = sort( unique(network$nmid) );

```

We have a full dataset, about 2000 movies and 29,601 actor-links.  We have about 17,150 unique actors.

We only have 29,601 entries and our matrix is 17149 * 1998 = 34,263,702 elements.  That is 0.0863% dense (or 99.9137% sparse).

AA would be an Actor-Actor matrix, we could perform ActorRank.
MM would be a Movie-Movie matrix, we could perform MovieRank.

This was of medium difficulty after including "C++" code into the framework.  About 20,000 for AA which was much slower than the 2,000 for MM.

```{r}
this.path = path.to.nascent;
setwd(this.path);

actorRank.modernEra = readRDS("actorRank2000.rds");
movieRank.modernEra = readRDS("movieRank2000.rds");
```


#### All time

Each year, there are top-50 most popular movies.  This determines the network.  The "Modern Era" has 15 actors or more on a movie.  Older movies had fewer, so the nature of "row-normalization" may benefit them.  At the same time, it should.  They were working with less.  It would be interesting to see if we have any money-data for these older movies.  

Marketing channels matter, unless the movies is "American Sniper"

MOJO was independent for a long time, now owned by IMDB.


<https://www.boxofficemojo.com/title/tt0052618/?ref_=bo_se_r_1>

<https://www.boxofficemojo.com/title/tt0120338/?ref_=bo_se_r_1>

<https://www.boxofficemojo.com/title/tt2179136/?ref_=bo_se_r_1>

```{r}
print("Ben-Hur (1959)");
# Ben Hur
loadInflationData();
adjustDollarForInflation(5016, 1959, 2000);  # int'l, south korea [Korean War = U.S. soldiers ?]
adjustDollarForInflation(74422622, 1959, 2000); # us

print("Titanic (1997) Original Release");
# Dec 18th release date ... cold time of year
loadInflationData();
adjustDollarForInflation(1250118282, 1998, 2000);  # int'l
adjustDollarForInflation(600788188, 1998, 2000); # us

print("American Sniper (2014) Original Release");
# Dec 25th release date
loadInflationData();
adjustDollarForInflation(197300000, 2015, 2000);  # int'l
adjustDollarForInflation(350126372, 2015, 2000); # us
```

I have heard at one point that "Titanic" of the modern era surpassed the (1959) "Ben-Hur" film for largest U.S. box-office ever (accounting for inflation) <https://www.imdb.com/search/title/?title_type=feature&year=1959-01-01,1959-12-31>. "Ben-Hur" made $5,016 in the international box office.  A different era for certain.

A modern film wants to "break-even" in the U.S. (spend $300 on budget, make that at the U.S. box office) and then make a billion dollars on the international market.  MCU moviez, StarWarz, Transformerz, and so on.

```{r}
network = merge(imdb.data$movies.df$cast, imdb.data$movies$popular50, by="ttid");
dim(network);
names(network);


my.ttids = sort( unique(network$ttid) );
my.nmids = sort( unique(network$nmid) );

n.ttid = length(unique(network$ttid));
n.nmid = length(unique(network$nmid));
```

We have a full dataset, over 5000 movies and 29,601 actor-links.  We have about 17,150 unique actors.

AA would be an Actor-Actor matrix, we could perform ActorRank.
MM would be a Movie-Movie matrix, we could perform MovieRank.

This was of extreme difficulty after including "C++" code into the framework.  (I would need to move to Xeon-processor servers and write more efficient code.)  About 35,242 for AA which was much slower than the 5,577  for MM.

```{r}
this.path = path.to.nascent;
setwd(this.path);

actorRank.allTime = readRDS("actorRank5000.rds");
movieRank.allTime = readRDS("movieRank5000.rds");
```
