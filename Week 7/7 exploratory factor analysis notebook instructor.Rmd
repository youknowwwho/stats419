---
title: 'R Notebook sandbox: Playing with Exploratory Factor Analysis (EFA)'
output:
  html_document:
    df_print: paged
    toc: true
    toc_float: true
    toc_depth: 4
    fig_caption: true
    number_sections: true
my-var: "monte"  # https://bookdown.org/yihui/rmarkdown/html-document.html
---
# Top of the world

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE);
knitr::opts_chunk$set(warning = FALSE);
knitr::opts_chunk$set(message = FALSE);

## this should knit, but I am running some IMDB stuff
## so I wasn't able to verify a final Knit.
## please let me know in the Discussion Board if you
## find any errors, and I will fix

# we don't want scientific notation
options(scipen  = 999);
```

# Exploratory Data Analysis (EDA)

Exploration is about discovery and looking for insights and themes.  One data aggregation approach is based on some "linear model" assumptions, but begins in the exploratory phase.  This is Exploratory Factor Analysis (EFA).

# Exploratory Factor Analysis (EFA)

Generally, EFA intends to aggregate features of the data.  This is common in the social sciences (psychology) where you ask a participant a lot of questions and you want to see if a group of these questions have similar responses.

For this introduction we will use the `personality-raw` data which is part of the latest version [0.1.4] of `library(humanVerseWSU);`




```{r, chunck-load-personality}



packageVersion("humanVerseWSU");  # ‘0.1.4’  [SHOULD BE THIS]

library(humanVerseWSU); 
# You need R tools for this to work:  https://cran.r-project.org/bin/windows/Rtools/
# You may want to see if you have the latest version...
# library(devtools);
# detach(package:humanVerseWSU);
# install_github("MonteShaffer/humanVerseWSU/humanVerseWSU"); 
# Choose (3) None to minimize headaches ....
# library(humanVerseWSU);


## this code was in correlation notebook, repeating here ...


personality.raw = readRDS( system.file("extdata", "personality-raw.rds", package="humanVerseWSU") );

cleanupPersonalityDataFrame = function(personality.raw)
  {
  df = removeColumnsFromDataFrame(personality.raw, "V00");
  dim(df);  # 838
  
  
  ywd.cols = c("year","week","day");
  ywd = convertDateStringToFormat( df$date_test,
                                    c("%Y","%W","%j"),  
                                    ywd.cols,
                                    "%m/%d/%Y %H:%M"
                                  );
  
  ndf = replaceDateStringWithDateColumns(df,"date_test",ywd);
  ndf = sortDataFrameByNumericColumns(ndf, ywd.cols, "DESC");
  ndf = removeDuplicatesFromDataFrame(ndf, "md5_email");
  
  dim(ndf); # 678
  ndf;
  }


personality.clean = cleanupPersonalityDataFrame(personality.raw);

### let's examine the data in total

personality.Vs = removeColumnsFromDataFrame(personality.clean,c("md5_email","year","week","day"));


X = personality.Vs;

Xs = scale(X);  # this is a good habit to get into, even if the data units are similar (as is the case here).

# because there are subject-level biases (e.g., a tendency for a user to answer questions higher/lower, maybe a "within-subject" scaling [row-level] followed by "within-item" scaling [col-level] may be appropriate.  Or maybe it removes some of the findings and their meaning?)

Xs = as.data.frame(Xs);

Xs;

```

## Check Conditions of Multivariate Reduction

Remember, during "exploration" there are really not any significant constraints.  In the previous notebook, I showed a few "normality" overlays of the PCA graphs for the countries (the elliptical forms). 

Some call these "diagnostic tests"

### KMO test

"The **Kaiser-Meyer-Olkin Measure of Sampling Adequacy** is a statistic that indicates the proportion of variance in your variables that might be caused by underlying factors. High values (close to 1.0) generally indicate that a factor analysis may be useful with your data. If the value is less than 0.50, the results of the factor analysis probably won't be very useful."

<https://www.ibm.com/support/knowledgecenter/SSLVMB_23.0.0/spss/tutorials/fac_telco_kmo_01.html>


```{r, chunck-check-kmo}



# this is the standard correlation matrix
Xs.corr = cor(Xs);

# library(KMO); # install.packages("KMO", dependencies=TRUE);  # not available for R == 4.0.2

library(REdaS); # install.packages("REdaS", dependencies=TRUE);

# https://www.rdocumentation.org/packages/REdaS/versions/0.9.3/topics/Kaiser-Meyer-Olkin-Statistics

Xs.KMO = KMOS(Xs);

str(Xs.KMO);


my.kmo = Xs.KMO$KMO;
my.kmo;

if(my.kmo >= 0.90)
  {
  print("marvelous!");
  } else if(my.kmo >= 0.80)
    {
    print("meritorious!");
    }  else if(my.kmo >= 0.70)
        {
        print("middling!");
        } else if(my.kmo >= 0.60)
            {
            print("mediocre!");
            }  else if(my.kmo >= 0.50)
                {
                print("miserable!");
                } else { 
                        print("mayhem!");
                        print("Oh snap!"); 
print("Kaiser-Meyer-Olkin (KMO) Test is a measure of how suited your data is for Factor Analysis. The test measures sampling adequacy for each variable in the model and for the complete model. The statistic is a measure of the proportion of variance among variables that might be common variance. The lower the proportion, the more suited your data is to Factor Analysis. <https://www.statisticshowto.com/kaiser-meyer-olkin/>");
                        }
```


### Sphericity test

"**Bartlett's test of sphericity** tests the hypothesis that your correlation matrix is an identity matrix, which would indicate that your variables are unrelated and therefore unsuitable for structure detection. Small values (less than 0.05) of the significance level indicate that a factor analysis may be useful with your data."

<https://www.ibm.com/support/knowledgecenter/SSLVMB_23.0.0/spss/tutorials/fac_telco_kmo_01.html>


```{r, chunck-check-bartlett}

# https://www.statology.org/bartletts-test-of-sphericity/
# Bartlett’s Test of Sphericity is not the same as Bartlett’s Test for Equality of Variances.

# this is the standard correlation matrix
Xs.corr = cor(Xs);

library(psych); # install.packages("psych", dependencies=TRUE);

Xs.bartlett = cortest.bartlett(Xs.corr, n = nrow(Xs));

str(Xs.bartlett);

# alpha level
alpha = 0.05;

if(Xs.bartlett$p.value < alpha)
  {
  print(paste0("Bartlett's test of sphericity ... pvalue < alpha ... ", Xs.bartlett$p.value , " < ", alpha, " ... \n CONCLUSION: we believe this data is likely suitable for factor analysis or PCA"));
  
  
  } else {  
          print("Oh snap!"); 
          print("To put this in layman's terms, the  variables in our dataset are fairly uncorrelated so a data reduction technique like PCA or factor analysis would have a hard time compressing these variables into linear combinations that are able to capture significant variance present in the data. <https://www.statology.org/bartletts-test-of-sphericity/>");
          }

# will be available in humanVerseWSU ... 0.1.4.1 (coming soon) ... 
# performBartlettSphericityTest(Xs);
# performBartlettSphericityTest(Xs.corr, n = nrow(Xs));
# myMeasure ... 

```


## What "the other guys" say

When you watch some YouTube videos or listen to the `machine-learning` crowd, they will suggest these tests are a waste of time.  You can believe that crowd if you want, but it may be a waste of time to perform analysis on data that isn't designed to be aggregated in this form.

**Note:** "Unsupervised learning" simply means "requires no human interaction."  I can program many of these data aggregation techniques to be completely automated (including developing a programmatic strategy to ascertain the idea number of factors):  _'I want my "AI algorithm" to run, so I don't want to perform a test and stop without executing.  I have so much data, at least show them something.'_  I emphasize this represents the "other guys", not good data analysts.

## How many factors

In this section, I will review various approaches to attempt to arrive at a conclusion of the number of factors (without merely looking at a scree plot like we did in `kmeans (wss)`.

### Old School: Very Simple Structure (VSS) 

```{r, chunck-nfactors-vss}

# pick a maximum number to examine
maxFactors = 8;  # I have 60 variables

Xs.vss = vss(Xs, n = maxFactors); # could also input Xs.corr, but you would want to include n.obs = nrow(Xs)

#str(Xs.vss);

Xs.vss.dataframe = cbind(1:maxFactors,Xs.vss$vss.stats[,c(1:3)]);

colnames(Xs.vss.dataframe) = c("Factors","d.f.","chisq","p.value");

Xs.vss.dataframe;

# we have so much data, it is saying we could use many different factors ...

# the choice for "4" seems to mean the "gain of a new factor" is minimal.

# https://personality-project.org/r/vss.html

# `__student_access__\sample_latex_files\Multivariate-2009\MonteShaffer_Stats519_HW5.pdf` # See HW5, pg 11 of a custom `multifactanal` table I constructed to use multiple criteria to assess optimal factors ...


```

### Scree: Using eigenvalues from Correlation 

```{r, chunck-nfactors-eigen}

Xs.corr.eigen = eigen(Xs.corr);  

library(nFactors);  # install.packages("nFactors", dependencies=TRUE);

# Basic
plotuScree(Xs.corr.eigen$values);
abline(h = 1, col="blue");

Xs.corr.eigen$values[Xs.corr.eigen$values > 1];
# technically 11 are greater than 1.
# 5-6 would seem reasonable based on what we see


# Steroids
nResults = nScree(eig = Xs.corr.eigen$values,
              aparallel = parallel(
                              subject = nrow(Xs), 
                              var = ncol(Xs) )$eigen$qevpea);

plotnScree(nResults, main="Component Retention Analysis");
# This is suggesting "6" based on Parallel Analysis and Optimal Coordinates ...

str(nResults);


# howManyFactorsToSelect(Xs);
```


### Eigenvalues > 1 plus parallel sampling

```{r, chunck-nfactors-parallel}
# I could loop over the data like I did in the `kmeans` notebook, but we can now consider functions that already do that ...

library(psych);  # install.packages("psych", dependencies=TRUE);

library(GPArotation);  # install.packages("GPArotation", dependencies=TRUE);

Xs.parallel = fa.parallel(Xs, fm = "minres", fa = "fa");

str(Xs.parallel);

# This is suggesting between 5-6

```


## Analysis with chosen factors (let's say 5)

```{r, chunck-factor5-summary}




round( psych::describe(Xs), digits=5);

Xs.factanal.5 = factanal(Xs, factors=5, rotation='none');
    # this uses "mle" method ...
    # ## rotation## #
    # varimax = assumes data is independent
    # promax = does a transform
    # none = nothing
# Xs.factanal.5 = factanal(covmat=Xs.corr, n.obs=nrow(Xs), factors=5, rotation='varimax');


# Uniqueness
head(Xs.factanal.5$uniquenesses);
# "Uniqueness" shows what???


# Map of Variables to Factors (Loadings)
print(Xs.factanal.5$loadings, digits=2, cutoff=0.25, sort=FALSE);

plot(Xs.factanal.5$loadings[,1:2], type="n");
text(Xs.factanal.5$loadings[,1:2],labels=names(Xs),cex=.7) # add variable names


print("Cool 3D graphs start here");
library(scatterplot3d); # install.packages("scatterplot3d", dependencies=TRUE);
library(rgl); # install.packages("rgl", dependencies=TRUE);





pchs = numeric(ncol(Xs));
  pchs[1:30] = 16;  # self
  pchs[31:60] = 17;  # other

# https://www.sessions.edu/color-calculator/
c.choices = c("steelblue", "#b46e46");
colors = character(ncol(Xs));
  colors[1:30] = c.choices[1];  # self
  colors[31:60] = c.choices[2];  # other


Xs.sp3d = scatterplot3d(Xs.factanal.5$loadings[,1:3],
              pch=pchs, color=colors,
              grid=TRUE, box=TRUE,
              type="p",
              angle=22
                );
legend(Xs.sp3d$xyz.convert(1.5, -0.75, 1),
        legend = c("Self","Other"),
        col =  c.choices,
        text.col = c.choices,
        pch = 16:17,
        bty = 'n'
      );  
  
# radius
rs = numeric(ncol(Xs));
  rs[1:30] = 0.05;  # self
  rs[31:60] = 0.03;  # other


# http://www.sthda.com/english/wiki/a-complete-guide-to-3d-visualization-device-system-in-r-r-software-and-data-visualization#basic-graph

print("Uncomment here to get more 3D in dynamic interactive form [WILL NOT KNIT]");
## uncomment, it will not KNIT
# rgl.open(); ## Open a new RGL device
# rgl.bg(color = "white");
# rgl.spheres(Xs.factanal.5$loadings[,1:3],
#             r = rs,
#             color = colors
#             );
# ## right click on a sphere
# identify3d(Xs.factanal.5$loadings[,1:3], labels = names(Xs), n = 5);


### alternatively
## uncomment, it will not KNIT
# plot3d(Xs.factanal.5$loadings[,1:3],
#         col=colors, box = FALSE,
#         type ="s", radius = rs
#       );






## TODO ... create a movie and store in directory ... create a PNG ...





## how about the raw data ...

round( psych::describe(X), digits=5);
X.factanal.5 = factanal(X, factors=5, rotation='none');
head(X.factanal.5$uniquenesses);
print(X.factanal.5$loadings, digits=2, cutoff=0.25, sort=FALSE);

plot(X.factanal.5$loadings[,1:2], type="n");
text(X.factanal.5$loadings[,1:2],labels=names(X),cex=.7) # add variable names

## notice any significant differences?  why or why not?  will this always be the case?  why is it the case in this situation?

```


# Personality

The above was to help you understand the key aspects of EFA.  Now we want to actually apply it `correctly` based on the data constraints we know.  Normally, I would take the Factor Loadings and try to understand them in relationship to the feature (V01).  

The loadings can be interpreted as correlations, so if the feature was "Happy" and it's value for Factor01 was 0.738, that means "Happy" is part of Factor01.  If the same feature was also linked to Factor04 with a value of -0.35, I could conclude that "Not Happy" is part of Factor04. (**See* `MonteShaffer_Stats519_HW5.pdf` Problems 2 and 3.)

The Personality data has two sections:

* Natural Self (Questions 1-30): How do you FEEL YOU REALLY ARE?

* Environmental Forces (Questions 31-60): How do you FEEL OTHERS EXPECT YOU TO ACT?

We need to do EFA on these two groups of features:  SELF (1-30) and OTHER (31-60)


## Optional FREE test

This summer I updated the old test (which is the data we are analyzing).  A beta-site of this new adminstration of the test can be found here:  <http://mpt.mshaffer.com/>

If interested, you can take the test for FREE and see some of the business-propositions I am developing as well.  The payment processing piece (BUY NOW with a CREDIT CARD) is not complete, and I hope to finish that over the Winter holiday.

You can take it, and you can invite your family and friends to take it.  All I would ask is that you have your family/friends use your WSU email (e.g., `YOURNAME@wsu.edu`) even though they should select their own gender, name, and age.  If they do (use your email), when the test is complete, you will receive an email with their results.

Below is some example links.  The `email` and `p-code` would change for your report.  This is my latest report. 

* Initial (partial report):  <http://mpt.mshaffer.com/report/mshaffer@mshaffer.com/p-5f7d9a7fedd5f/>

* Initial (partial report with exploding time offer):  <http://mpt.mshaffer.com/report/mshaffer@mshaffer.com/p-5f7d9a7fedd5f/?has-paid=33>

* Full Report (as-if you paid): <http://mpt.mshaffer.com/report/mshaffer@mshaffer.com/p-5f7d9a7fedd5f/?has-paid=true>

As you can see from the above, I just took the test and spent less than 4 minutes on it.  So the accuracy is probably not perfect, but it is somewhat accurate. I would suggest you take the test when you are in a "reflective mood" and devote at least 15 minutes to it. 

I would say my natural self is a D-4 or D-6.  This is a version of an older report from 2018 <http://www.mshaffer.com/arizona/Monte_Shaffer__RleeFtjanggAAOn6m00.pdf>, that was true at the time (I was working with a lot of young CS interns/hires working on an ECE project <https://nsf.gov/awardsearch/showAward?AWD_ID=1819997> to develop technologies to automate exercise routines for persons with Parkinson's Disease).


## Self: (1-30)

The objective is to assess how many factors to extra for the data.  Once determined, a summary of the factors and their relationship to the original words on the personality test.

--DO SOMETHING HERE--

```{r, chunck-personality-self}
Xs.self = Xs[,1:30];

Xs.self;


library(devtools);
source_url("https://raw.githubusercontent.com/MonteShaffer/humanVerseWSU/master/humanVerseWSU/R/functions-EDA.R");
Xs.self.how.many = howManyFactorsToSelect(Xs.self);
Xs.self.EFA.factanal = perform.EFA(Xs.self, 6, which="factanal",
                              rotation="varimax", scores = "regression");

Xs.self.EFA.fa       = perform.EFA(Xs.self, 6, which="fa",
                              rotation="oblimin", scores = "regression", fa.fm="ml");

```

## Other: (31-60)

--DO SOMETHING HERE--

```{r, chunck-personality-other}
Xs.other = Xs[,31:60];

Xs.other;

library(devtools);
source_url("https://raw.githubusercontent.com/MonteShaffer/humanVerseWSU/master/humanVerseWSU/R/functions-EDA.R");
Xs.other.how.many = howManyFactorsToSelect(Xs.other);
Xs.other.EFA.factanal = perform.EFA(Xs.other, 6, which="factanal",
                              rotation="varimax", scores = "regression");

Xs.other.EFA.fa       = perform.EFA(Xs.other, 6, which="fa",
                              rotation="oblimin", scores = "regression", fa.fm="ml");

```



